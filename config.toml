[args]
strategy = "train" # train | test
data_path = "data.xlsx" # xlsx、csv、json、parquet
train_column = "text" # 训练文件的列名
max_new_tokens = 100 # label的最大值

[Pretrained]
model_name = "Qwen2.5-7B" # 预训练大模型的文件夹名字
load_in_4bit = false # 启动4K量化 2-4%
fast_inference = false # 加速推理
max_seq_length = 2048 # 输入的最大token

[Envs]
CUDA_VISIBLE_DEVICES = "0"
HF_ENDPOINT = "https://hf-mirror.com" # 拉模型的国内代理
UNSLOTH_RETURN_LOGITS = "1"

[SFTConfig]
per_device_train_batch_size = 2 # 参数值调高 训练的速度快 显存高
gradient_accumulation_steps = 1 # 如果显存低 那么将这个参数值调高 训练的速度将慢
num_train_epochs = 3
learning_rate = 1e-5
logging_steps = 100
save_total_limit = 3
output_dir = "output" # 训练的时候 这个目录是保持训练后的模型地址。如果测试的时候，那么这个目录就是加载你的模型
packing = false

[Peft]
r = 16
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
lora_alpha = 16
lora_dropout = 0
bias = "none"
use_gradient_checkpointing = "unsloth"
use_rslora = false
